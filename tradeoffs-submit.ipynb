{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Space-Data Tradeoffs for Bleichenbacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1. Repeated HGJ-SS ($A_{4,M_1},A_{4,M_2},\\ldots,A_{4,M_r}$)\n",
    "Suppose we are searching $K=4^r$-list sums. This straightforward algorithm repeatedly appplies Howgrave-Graham-Joux's Schroeppel-Shamir (HGJ-SS, denoted by $A_{4,M}$ in Dinur) for $r$ times.\n",
    "We use the following notations:\n",
    "\n",
    "  - $L_i=2^{\\ell_i}$: Upper bound of the size of samples after i-th round. $L_0$ and $L_r$ corresponds to the upper of input and output samples respectively.\n",
    "  - $M_i=2^{m_i}=2^{a_i+2}$: Number of samples after the i-th round, consisting of 4 lists of size $2^{a_i}$. $M_0 $ and $M_r$ corresponds to the number of input and output samples respectively.\n",
    "  - $v_i\\in[0,a_i]$: A parameter deciding how many iterations of the initial collision search to be executed in HGJ-SS.\n",
    "  - $T_i=2^{t_i}=2^{a_i+v_i}$: Time complexity for the i-th round. \n",
    "  - $N_i=2^{n_i}$: $n_i$ bits are nullified after the i-th round.\n",
    "\n",
    "If $v_i=a_i$ then we do the initial collision search exhausitively. In that case we find in total $M_{i+1}=2^{4a_i}/N_i$ samples of which $n_i$ bits are nullified in time $T_i=2^{2a_i}$. If we terminate the collision search at some $v_i$ then we only get $M_{i+1}=2^{4a_i}/(N_i2^{a_i-v_i})$ samples in time $T_i=2^{a_i+v_i}$.\n",
    "We thus obtain the trade-off formula for each round:\n",
    "    $$m_{1}+n_0 = 3a_0+v_0 \\\\ \\vdots \\\\ m_{i+1}+n_i = 3a_i+v_i \\\\ \\vdots \\\\ m_{r}+n_{r-1} = 3a_{r-1}+v_{r-1}.$$\n",
    "\n",
    "Put differently, the formula can be written as\n",
    "    $$M_{i+1}N_{i}=T_iM_i^2/2^4$$\n",
    "\n",
    "which is essentially same as the trade-off for $A_{4,M_{i+1}}$ by Dinur.\n",
    "The main difference with Dinur's framework is that we do not force the intermediate samples to be balanced (i.e. not necessarity $m_0=\\ldots=m_{r-1}$) to allow the flexible amplification of samples.\n",
    "Furthermore, we have several constraints.\n",
    "\n",
    "  - $m_{i+1}\\leq 2a_i$: HGJ-SS only allows to go through $2^{2a_i}$ in total even if we decide not to nullify at all after the initial collision phase. We could drop this constraint by executing the exhaustive search instead of HGJ-SS during the first phase, so that the number of samples can be amplified to at most $2^{4a_i}$ in $2^{4a_i}$ time.\n",
    "  - $\\ell_r = \\ell_0-\\sum_{i\\in[0,r-1]}n_i \\leq \\ell_{FFT}$: \"Small\" condition from Bleichenbacher.\n",
    "  - $|\\text{Bias}|^K \\geq \\text{slack}/\\sqrt{2^{m_r}}\\iff m_r \\geq 2(\\log(\\text{slack})-4^r\\log(\\text{Bias}))$: \"Sparse\" condition from Bleichenbacher.\n",
    "\n",
    "To get the optimal time complexity we should balance $T_i$ for each round and hence assume another constaint\n",
    "    $$ a_0+v_0 = \\ldots a_{r-1}+v_{r-1}.$$\n",
    "\n",
    "## Case 2. Multi-layer HGJ-SS ($A^4_{K,M_r}$)\n",
    "This is slightly different from the prior case because the algorithm first divides its input into $K$-list and iteratevily applies HGJ-SS. The advantage of this approach is that the resulting samples are strictly $K$-list sums, while repeated HGJ-SS doesn't gurantee it. This property is important when we amplify the input size. However, the time complexity seems slightly worse (about a factor of 4) than repeated HGJ-SS. \n",
    "For each layer the algorithm works as follows.\n",
    "\n",
    " - For the layer 0, given $K$-list of size $2^{a_0}$ as input apply HGJ-SS to each 4-list to create $K/4$ lists of length $2^{a_1}$.\n",
    " - For the layer 1, given $K/4$-list of size $2^{a_1}$ as input apply HGJ-SS to each 4-list to create $K/4^2$ lists of length $2^{a_2}$.\n",
    " - $\\ldots$\n",
    " - For the layer $r-1$, given $4$-list of size $2^{a_{r-1}}$ as input apply HGJ-SS to each 4-list to create a single list of length $2^{a_r}$.\n",
    "\n",
    "The trade-off formula is almost same as repeated HGJ-SS except that left hand side has $a$ instead of $m$:\n",
    "\n",
    "$$a_{i+1}+n_i = 3a_i+v_i.$$\n",
    "\n",
    "Also the total number of samples for each layer $i$ is now\n",
    "\n",
    "$$M_i=2^{m_i}=2^{a_i+2(r-i)}.$$\n",
    " \n",
    "## Case 3. BCJ ($A_{16,M_1}$)\n",
    "This is based on Becker-Coron-Joux's algorithm (BCJ) that directly solves 16-list sum per round. For simplicity we only consider a single round. \n",
    "Suppose we are given 16 lists of size $2^{a_0}$ (and thus $M_0=2^{a_0+4}$). \n",
    "For each set of 4 lists, the BCJ's initial collision search looks at the top $3a_i$ bits of 4-sum and collects linear combinations that have $c_1,c_2,c_3,c_4$ respectively, where $c_i$'s cancel out when they're summed later.\n",
    "There are in total $9a_i$ combinations of such $c_i$'s.\n",
    "Finally, we apply the HGJ-SS to those 4 lists to search collisions in the remaining bits\n",
    "As we iterate over $9a_0$ combinations of $c_i$'s the total time complexity is $T_i=2^{2a_i+w_i}$ using the following parameter.\n",
    "\n",
    "- $w_i\\in[0,9a_i]$: A parameter deciding how many iterations of the initial collision search to be executed in BCJ.\n",
    "\n",
    "The trade-off formula is \n",
    "$$m_{i+1}+n_i = 7a_i+w_i$$\n",
    "because for each iteration $3a_i$ bits are nullified by the initial collision search and another $3a_i$ bits are nullified by the HGJ-SS. \n",
    "The additinal constraints are as follows, where the number of rounds is denoted by $r$ and we set $K=16^r$ since each round of BCJ finds 16-list sum solutions.\n",
    "\n",
    "  - $m_{1}\\leq 11a_0$: The HGJ-SS goes through at most $2^{2a_0}$ linear combinations and it is iterated $2^{9a_0}$ times. \n",
    "  - $\\ell_r = \\ell_0-\\sum_{i\\in[0,r-1]}n_i \\leq \\ell_{FFT}$: \"Small\" condition from Bleichenbacher.\n",
    "  - $|\\text{Bias}|^K \\geq \\text{slack}/\\sqrt{2^{m_r}}\\iff m_r \\geq 2(\\log(\\text{slack})-16^{r}\\log(\\text{Bias}))$: \"Sparse\" condition from Bleichenbacher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Programming for HGJ-repeat, HGJ-layer, BCJ-repeat, BCJ-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitbias(b):\n",
    "    return float(sin(pi/2^b)*2^b/pi)\n",
    "\n",
    "def dinursolver(bias, groupsize, rounds, memory=28, fftsize=30, numsig=None, time=None, slack=8, algorithm=\"HGJ-repeat\", known=0, amplifyfirst=False, streamout=False):\n",
    "    # Input samples: m[0]\n",
    "    # Initial bit-lengh: l[0]   \n",
    "    p = MixedIntegerLinearProgram()\n",
    "    m = p.new_variable(real=True, nonnegative=True) # samples = 2^m\n",
    "    a = p.new_variable(real=True, nonnegative=True) # size of each list\n",
    "    n = p.new_variable(real=True, nonnegative=True) # nullified bits\n",
    "    l = p.new_variable(real=True, nonnegative=True) # current bit length\n",
    "    v = p.new_variable(real=True, nonnegative=True) # parameter\n",
    "    w = p.new_variable(real=True, nonnegative=True) # parameter\n",
    "    t = p.new_variable(real=True, nonnegative=True) # time for round i\n",
    "    f = p.new_variable(real=True, nonnegative=True) # filter\n",
    "    sample = p.new_variable(real=True, nonnegative=True)\n",
    "    # intermediate parameters for BCJ\n",
    "    aprime = p.new_variable(real=True, nonnegative=True) # size of each list\n",
    "    mprime = p.new_variable(real=True, nonnegative=True) # size of each list\n",
    "    nprime = p.new_variable(real=True, nonnegative=True) # nullified bits\n",
    "    vprime = p.new_variable(real=True, nonnegative=True) # parameter\n",
    "    tprime = p.new_variable(real=True, nonnegative=True) # time for round i\n",
    "    \n",
    "    # K-list sum to reduce\n",
    "    r = rounds\n",
    "    for i in range(r):        \n",
    "        if algorithm == \"HGJ-repeat\": \n",
    "            p.add_constraint(l[i+1] == l[i] - n[i])\n",
    "            if i == 0 and amplifyfirst:\n",
    "                p.add_constraint(m[i+1] == v[i] - n[i])\n",
    "                p.add_constraint(t[i] == v[i])\n",
    "                p.add_constraint(v[i] <= 4*a[i])\n",
    "                p.add_constraint(m[i] == a[i] + 2)\n",
    "                p.add_constraint(m[i+1] <= a[i] + 4)\n",
    "            else:\n",
    "                p.add_constraint(m[i+1] == 3*a[i] + v[i] - n[i])\n",
    "                p.add_constraint(t[i] == a[i] + v[i])\n",
    "                p.add_constraint(v[i] <= a[i])\n",
    "                p.add_constraint(m[i] == a[i] + 2) \n",
    "                p.add_constraint(m[i+1] <= 2*a[i])\n",
    "            \n",
    "        if algorithm == \"HGJ-layer\":\n",
    "            p.add_constraint(l[i+1] == l[i] - n[i])\n",
    "            if i == 0 and amplifyfirst:\n",
    "                p.add_constraint(a[i+1] == v[i] - n[i])\n",
    "                p.add_constraint(t[i] == v[i] + 2*(r-i-1)) # r = 3 if K=4^3 for instance, then do search 64/4=16 times\n",
    "                p.add_constraint(v[i] <= 4*a[i])\n",
    "                p.add_constraint(m[i] == a[i] + 2*(r-i))\n",
    "                p.add_constraint(a[i+1] <= a[i] + 4)\n",
    "            else:\n",
    "                p.add_constraint(a[i+1] == 3*a[i] + v[i] - n[i])\n",
    "                p.add_constraint(t[i] == a[i] + v[i] + 2*(r-i-1))\n",
    "                p.add_constraint(v[i] <= a[i])\n",
    "                p.add_constraint(m[i] == a[i] + 2*(r-i))\n",
    "                p.add_constraint(a[i+1] <= 2*a[i])\n",
    "                \n",
    "        if algorithm == \"BCJ-repeat\":\n",
    "            p.add_constraint(l[i+1] == l[i] - n[i])\n",
    "            if i == 0 and amplifyfirst:\n",
    "                p.add_constraint(m[i+1] == v[i] - n[i])\n",
    "                p.add_constraint(t[i] == v[i])\n",
    "                p.add_constraint(v[i] <= 4*a[i])\n",
    "                p.add_constraint(m[i] == a[i] + 2)           \n",
    "                p.add_constraint(m[i+1] <= a[i] + 4)\n",
    "            else:\n",
    "                p.add_constraint(m[i+1] == 7*a[i] + w[i] - n[i])\n",
    "                p.add_constraint(t[i] == 2*a[i] + w[i])\n",
    "                p.add_constraint(w[i] <= 9*a[i])\n",
    "                p.add_constraint(m[i] == a[i] + 4)                \n",
    "                p.add_constraint(m[i+1] <= 11*a[i])\n",
    "                \n",
    "        if algorithm == \"BCJ-layer\":\n",
    "            p.add_constraint(l[i+1] == l[i] - n[i])\n",
    "            if i == 0 and amplifyfirst:\n",
    "                p.add_constraint(a[i+1] == v[i] - n[i])\n",
    "                p.add_constraint(t[i] == v[i] + 4*(r-i-1)) # r = 2 if K=16^1.5 for instance, then do search 64/4=16 times\n",
    "                p.add_constraint(v[i] <= 4*a[i])\n",
    "                p.add_constraint(m[i] == a[i] + 4*(r-i))\n",
    "                p.add_constraint(a[i+1] <= a[i] + 4)\n",
    "            else:\n",
    "                p.add_constraint(a[i+1] == 7*a[i] + w[i] - n[i])\n",
    "                p.add_constraint(t[i] == 2*a[i] + w[i] + 4*(r-i-1))\n",
    "                p.add_constraint(w[i] <= 9*a[i])\n",
    "                p.add_constraint(m[i] == a[i] + 4*(r-i))\n",
    "                p.add_constraint(a[i+1] <= 11*a[i])\n",
    "                \n",
    "    # Resulting samples\n",
    "    if algorithm == \"HGJ-repeat\": \n",
    "        p.add_constraint(m[r] == a[r]+2)\n",
    "    if algorithm == \"HGJ-layer\":\n",
    "        p.add_constraint(m[r] == a[r])\n",
    "    if algorithm == \"BCJ-repeat\":\n",
    "        p.add_constraint(m[r] == a[r]+4)\n",
    "    if algorithm == \"BCJ-layer\":\n",
    "        p.add_constraint(m[r] == a[r])\n",
    "        \n",
    "    # Bleichenbacher-1: Minimal number of samples necessary to distinguish from noise\n",
    "    noisefloor = 0\n",
    "    if algorithm == \"HGJ-repeat\" or algorithm == \"HGJ-layer\": \n",
    "        noisefloor = numerical_approx(-2*4^r*log(bias,2) + 2*log(slack,2))\n",
    "    if algorithm == \"BCJ-repeat\" or algorithm == \"BCJ-layer\": \n",
    "        if amplifyfirst:\n",
    "            noisefloor = numerical_approx(-2*16^(r-0.5)*log(bias,2) + 2*log(slack,2))\n",
    "        else:\n",
    "            noisefloor = numerical_approx(-2*16^r*log(bias,2) + 2*log(slack,2))\n",
    "    p.add_constraint(m[r] >= noisefloor)\n",
    "    \n",
    "    # Bleichenbacher-2: Size constraints\n",
    "    p.add_constraint(l[0] == groupsize - f[0])\n",
    "    p.add_constraint(l[r] <= fftsize + known)\n",
    "\n",
    "    # Goals\n",
    "    if time is None:\n",
    "        for i in range(r-1):\n",
    "            p.add_constraint(t[i] == t[i+1]) # equalize time for each round\n",
    "        p.set_objective(-t[0]) \n",
    "    else:\n",
    "        for i in range(r):\n",
    "            p.add_constraint(t[i] <= time)\n",
    "\n",
    "    if memory is None:\n",
    "        for i in range(r-1):\n",
    "            p.add_constraint(m[i] == m[i+1]) # equalize memory for each round\n",
    "        if not streamout:\n",
    "            p.add_constraint(m[r-1] == m[r])\n",
    "        p.set_objective(-m[0])\n",
    "    else:\n",
    "        for i in range(r):\n",
    "            p.add_constraint(m[i] <= memory)\n",
    "        if not streamout:\n",
    "            p.add_constraint(m[r] <= memory)\n",
    "\n",
    "    if fftsize is None:\n",
    "        p.set_objective(-l[r])\n",
    "    else:\n",
    "        p.add_constraint(l[r] <= fftsize + known)\n",
    "\n",
    "    if numsig is None:\n",
    "        p.set_objective(-(m[0] + f[0]))\n",
    "    else:\n",
    "        p.add_constraint(m[0] + f[0] == numsig)\n",
    "                        \n",
    "    try:\n",
    "        p.solve()\n",
    "    except:\n",
    "        print \"Constraints impossible to satisfy!\"\n",
    "        return (0, 0)\n",
    "\n",
    "    memcost = max([p.get_values(m[i]) for i in range(r)] + [p.get_values(mprime[i]) for i in range(r)])\n",
    "    redtime = max([p.get_values(t[i]) for i in range(r)] + [p.get_values(tprime[i]) for i in range(r)])\n",
    "    fftcost = p.get_values(l[r])\n",
    "    \n",
    "    print \"Solved problem with %s!\" % algorithm \n",
    "    if amplifyfirst:\n",
    "        print \" (with amplification first)\"\n",
    "    print \"===========================\"\n",
    "    print \"Initial signatures: 2^%05.2f with %d bits of sk known\" % (p.get_values(m[0])+p.get_values(f[0]), known)\n",
    "    print \"Memory cost:        2^%05.2f\" % memcost\n",
    "    print \"Reduction time:     2^%05.2f\" % redtime\n",
    "    print \"FFT size:           2^%05.2f\" % (fftcost - known)\n",
    "    print \"Filtered bits:      %05.2f\"   % p.get_values(f[0])\n",
    "    print \"===========================\"\n",
    "    for i in range(r):\n",
    "        print \"Round %d:\" % i\n",
    "        print \"    Input:  2^%05.2f samples of %6.2f bits\" % (p.get_values(m[i]), p.get_values(l[i]))\n",
    "        print \"    Output: 2^%05.2f samples of %6.2f bits\" % (p.get_values(m[i+1]), p.get_values(l[i+1]))\n",
    "        print \"                      Nullified %5.2f bits\" % (p.get_values(n[i])+p.get_values(nprime[i]))\n",
    "        print \"    t[%d] = %05.2f, t'[%d] = %05.2f\" % (i, p.get_values(t[i]), i, p.get_values(tprime[i]))\n",
    "        print \"    a[%d] = %05.2f, a'[%d] = %05.2f\" % (i, p.get_values(a[i]), i, p.get_values(aprime[i]))\n",
    "        print \"    v[%d] = %05.2f, v'[%d] = %05.2f\" % (i, p.get_values(v[i]), i, p.get_values(vprime[i]))\n",
    "        print \"    m[%d] = %05.2f, m'[%d] = %05.2f\" % (i, p.get_values(m[i]), i, p.get_values(mprime[i]))\n",
    "        print \"    n[%d] = %05.2f, n'[%d] = %05.2f\" % (i, p.get_values(n[i]), i, p.get_values(nprime[i]))\n",
    "        if algorithm == \"BCJ-repeat\" or algorithm == \"BCJ-layer\":\n",
    "            print \"    w[%d] = %05.2f\" % (i, p.get_values(w[i]))\n",
    "        \n",
    "    if algorithm == \"BCJ-repeat\" or algorithm == \"BCJ-layer\":\n",
    "        if amplifyfirst:\n",
    "            print \"Expected peak = %01.10f\" % (bias^(16^(rounds-0.5)))\n",
    "        else:\n",
    "            print \"Expected peak = %01.10f\" % (bias^(16^rounds))\n",
    "    else:\n",
    "        print \"Expected peak = %01.10f\" % (bias^(4^rounds))\n",
    "    print \"Average noise = %01.10f\" % (1/2^(p.get_values(m[r])/2))\n",
    "    print \"Noise * slack = %01.10f\" % (slack/2^(p.get_values(m[r])/2))\n",
    "    return (p.get_values(m[0])+p.get_values(f[0]), redtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Performance Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirmed\n",
    "def estimate(tcomp, threads_per_node, nodes, inputsize, resultsize, comb_A_size, scale1, scale2, scale3, r=1):\n",
    "    tcomp_base = 49.89\n",
    "    # Only for range reduction for-loop per round.\n",
    "    # Actual time in AWS, 24 nodes * 96 threads\n",
    "    # Including sort and saving it was 24*2+6.3=54.3 hours\n",
    "    cpuh_base = 59904 \n",
    "    threads = threads_per_node*nodes\n",
    "    cpuh = cpuh_base*2^(tcomp-tcomp_base)\n",
    "    threadh = cpuh/threads\n",
    "    threadd = threadh/24\n",
    "    # LRComb64: uint64_t + uint32_t * 2 \n",
    "    mem_combs = 1.2 * (8 + 4*2) * (2^inputsize * scale1 / 2) * threads_per_node\n",
    "    # L1 R1 L2 R2: uint128_t \n",
    "    mem_samples = 16 * 2^inputsize \n",
    "    # Index: uint32_t * 4 + uint8_t\n",
    "    mem_combA = (4*4 + 1) * comb_A_size * threads_per_node\n",
    "    # Index: uint32_t * 4 + uint8_t\n",
    "    mem_subresult = (4*4 + 1) *  (2^resultsize * scale2 * scale3) / nodes\n",
    "    print \"2^%.1f * %d round = %1.f h/CPU = %.2f h(=%.1f days)/%d threads\" % (tcomp,r,cpuh*r,threadh*r, threadd*r, threads)\n",
    "    print \"%.1f GB/node\" %  ((mem_combs + mem_samples + mem_combA + mem_subresult)/2^30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2^40.0 * 2 round = 126 h/CPU = 0.05 h(=0.0 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^41.0 * 2 round = 253 h/CPU = 0.11 h(=0.0 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^42.0 * 2 round = 505 h/CPU = 0.22 h(=0.0 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^43.0 * 2 round = 1010 h/CPU = 0.44 h(=0.0 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^44.0 * 2 round = 2020 h/CPU = 0.88 h(=0.0 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^45.0 * 2 round = 4041 h/CPU = 1.75 h(=0.1 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^46.0 * 2 round = 8081 h/CPU = 3.51 h(=0.1 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^47.0 * 2 round = 16163 h/CPU = 7.01 h(=0.3 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^48.0 * 2 round = 32325 h/CPU = 14.03 h(=0.6 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^49.0 * 2 round = 64650 h/CPU = 28.06 h(=1.2 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^50.0 * 2 round = 129300 h/CPU = 56.12 h(=2.3 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^51.0 * 2 round = 258600 h/CPU = 112.24 h(=4.7 days)/2304 threads\n",
      "473.8 GB/node\n",
      "2^52.0 * 2 round = 517201 h/CPU = 224.48 h(=9.4 days)/2304 threads\n",
      "473.8 GB/node\n"
     ]
    }
   ],
   "source": [
    "for i in range(40,53):\n",
    "    estimate(i, 96, 24, 29, 29, 5000, 1.01, 1.01, 1.1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIST P-192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacking P192 with 1-bit bias (with time optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved problem with HGJ-repeat!\n",
      "===========================\n",
      "Initial signatures: 2^29.00 with 0 bits of sk known\n",
      "Memory cost:        2^29.00\n",
      "Reduction time:     2^50.92\n",
      "FFT size:           2^38.00\n",
      "Filtered bits:      00.00\n",
      "===========================\n",
      "Round 0:\n",
      "    Input:  2^29.00 samples of 192.00 bits\n",
      "    Output: 2^29.00 samples of 116.08 bits\n",
      "                      Nullified 75.92 bits\n",
      "    t[0] = 50.92, t'[0] = 00.00\n",
      "    a[0] = 27.00, a'[0] = 00.00\n",
      "    v[0] = 23.92, v'[0] = 00.00\n",
      "    m[0] = 29.00, m'[0] = 00.00\n",
      "    n[0] = 75.92, n'[0] = 00.00\n",
      "Round 1:\n",
      "    Input:  2^29.00 samples of 116.08 bits\n",
      "    Output: 2^26.85 samples of  38.00 bits\n",
      "                      Nullified 78.08 bits\n",
      "    t[1] = 50.92, t'[1] = 00.00\n",
      "    a[1] = 27.00, a'[1] = 00.00\n",
      "    v[1] = 23.92, v'[1] = 00.00\n",
      "    m[1] = 29.00, m'[1] = 00.00\n",
      "    n[1] = 78.08, n'[1] = 00.00\n",
      "Expected peak = 0.0007279172\n",
      "Average noise = 0.0000909896\n",
      "Noise * slack = 0.0007279172\n",
      "2^51.0 * 2 round = 258600 h/CPU = 112.24 h(=4.7 days)/2304 threads\n",
      "492.6 GB/node\n"
     ]
    }
   ],
   "source": [
    "# Full attack on 1-bit bias without error; HGJ-repeat\n",
    "dinursolver(bitbias(1), 192, 2, memory=29, fftsize=38, numsig=29, time=None, slack=8, algorithm=\"HGJ-repeat\")\n",
    "# Performance estimates in AWS\n",
    "estimate(51, 96, 24, 29, 30, 5000, 1.05, 1.01, 1.1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved problem with HGJ-repeat!\n",
      "===========================\n",
      "Initial signatures: 2^35.00 with 0 bits of sk known\n",
      "Memory cost:        2^29.00\n",
      "Reduction time:     2^49.89\n",
      "FFT size:           2^37.00\n",
      "Filtered bits:      06.00\n",
      "===========================\n",
      "Round 0:\n",
      "    Input:  2^29.00 samples of 186.00 bits\n",
      "    Output: 2^29.00 samples of 111.11 bits\n",
      "                      Nullified 74.89 bits\n",
      "    t[0] = 49.89, t'[0] = 00.00\n",
      "    a[0] = 27.00, a'[0] = 00.00\n",
      "    v[0] = 22.89, v'[0] = 00.00\n",
      "    m[0] = 29.00, m'[0] = 00.00\n",
      "    n[0] = 74.89, n'[0] = 00.00\n",
      "Round 1:\n",
      "    Input:  2^29.00 samples of 111.11 bits\n",
      "    Output: 2^29.78 samples of  37.00 bits\n",
      "                      Nullified 74.11 bits\n",
      "    t[1] = 49.89, t'[1] = 00.00\n",
      "    a[1] = 27.00, a'[1] = 00.00\n",
      "    v[1] = 22.89, v'[1] = 00.00\n",
      "    m[1] = 29.00, m'[1] = 00.00\n",
      "    n[1] = 74.11, n'[1] = 00.00\n",
      "Expected peak = 0.0005268648\n",
      "Average noise = 0.0000329290\n",
      "Noise * slack = 0.0005268648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.0, 49.89027960210937)"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attack on 1-bit bias with with 1% error; HGJ-repeat\n",
    "p = 0.99\n",
    "dinursolver(bitbias(1)*(p-(1-p)), 192, 2, memory=29, fftsize=37, numsig=35, time=None, slack=16, algorithm=\"HGJ-repeat\",streamout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved problem with HGJ-layer!\n",
      "===========================\n",
      "Initial signatures: 2^35.00 with 0 bits of sk known\n",
      "Memory cost:        2^29.00\n",
      "Reduction time:     2^51.89\n",
      "FFT size:           2^37.00\n",
      "Filtered bits:      06.00\n",
      "===========================\n",
      "Round 0:\n",
      "    Input:  2^29.00 samples of 186.00 bits\n",
      "    Output: 2^29.00 samples of 113.11 bits\n",
      "                      Nullified 72.89 bits\n",
      "    t[0] = 51.89, t'[0] = 00.00\n",
      "    a[0] = 25.00, a'[0] = 00.00\n",
      "    v[0] = 24.89, v'[0] = 00.00\n",
      "    m[0] = 29.00, m'[0] = 00.00\n",
      "    n[0] = 72.89, n'[0] = 00.00\n",
      "Round 1:\n",
      "    Input:  2^29.00 samples of 113.11 bits\n",
      "    Output: 2^29.78 samples of  37.00 bits\n",
      "                      Nullified 76.11 bits\n",
      "    t[1] = 51.89, t'[1] = 00.00\n",
      "    a[1] = 27.00, a'[1] = 00.00\n",
      "    v[1] = 24.89, v'[1] = 00.00\n",
      "    m[1] = 29.00, m'[1] = 00.00\n",
      "    n[1] = 76.11, n'[1] = 00.00\n",
      "Expected peak = 0.0005268648\n",
      "Average noise = 0.0000329290\n",
      "Noise * slack = 0.0005268648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.0, 51.89027960210935)"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attack on 1-bit bias with with 1% error; HGJ-layer\n",
    "p = 0.99\n",
    "dinursolver(bitbias(1)*(p-(1-p)), 192, 2, memory=29, fftsize=37, numsig=35, time=None, slack=16, algorithm=\"HGJ-layer\", streamout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved problem with BCJ-repeat!\n",
      "===========================\n",
      "Initial signatures: 2^35.00 with 0 bits of sk known\n",
      "Memory cost:        2^29.00\n",
      "Reduction time:     2^53.78\n",
      "FFT size:           2^37.00\n",
      "Filtered bits:      06.00\n",
      "===========================\n",
      "Round 0:\n",
      "    Input:  2^29.00 samples of 186.00 bits\n",
      "    Output: 2^29.78 samples of  37.00 bits\n",
      "                      Nullified 149.00 bits\n",
      "    t[0] = 53.78, t'[0] = 00.00\n",
      "    a[0] = 25.00, a'[0] = 00.00\n",
      "    v[0] = 00.00, v'[0] = 00.00\n",
      "    m[0] = 29.00, m'[0] = 00.00\n",
      "    n[0] = 149.00, n'[0] = 00.00\n",
      "    w[0] = 03.78\n",
      "Expected peak = 0.0005268648\n",
      "Average noise = 0.0000329290\n",
      "Noise * slack = 0.0005268648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.0, 53.78055920421875)"
      ]
     },
     "execution_count": 795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attack on 1-bit bias with with 1% error; BCJ-repeat\n",
    "p = 0.99\n",
    "dinursolver(bitbias(1)*(p-(1-p)), 192, 1, memory=29, fftsize=37, numsig=35, time=None, slack=16, algorithm=\"BCJ-repeat\",streamout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved problem with BCJ-layer!\n",
      "===========================\n",
      "Initial signatures: 2^35.00 with 0 bits of sk known\n",
      "Memory cost:        2^29.00\n",
      "Reduction time:     2^53.78\n",
      "FFT size:           2^37.00\n",
      "Filtered bits:      06.00\n",
      "===========================\n",
      "Round 0:\n",
      "    Input:  2^29.00 samples of 186.00 bits\n",
      "    Output: 2^29.78 samples of  37.00 bits\n",
      "                      Nullified 149.00 bits\n",
      "    t[0] = 53.78, t'[0] = 00.00\n",
      "    a[0] = 25.00, a'[0] = 00.00\n",
      "    v[0] = 00.00, v'[0] = 00.00\n",
      "    m[0] = 29.00, m'[0] = 00.00\n",
      "    n[0] = 149.00, n'[0] = 00.00\n",
      "    w[0] = 03.78\n",
      "Expected peak = 0.0005268648\n",
      "Average noise = 0.0000329290\n",
      "Noise * slack = 0.0005268648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.0, 53.78055920421875)"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attack on 1-bit bias with with 1% error; BCJ-layer\n",
    "p = 0.99\n",
    "dinursolver(bitbias(1)*(p-(1-p)), 192, 1, memory=29, fftsize=37, numsig=35, time=None, slack=16, algorithm=\"BCJ-layer\",streamout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sect163r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacking sect163r1 with 1-bit bias (with data optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved problem with HGJ-repeat!\n",
      "===========================\n",
      "Initial signatures: 2^22.96 with 0 bits of sk known\n",
      "Memory cost:        2^29.00\n",
      "Reduction time:     2^45.00\n",
      "FFT size:           2^35.00\n",
      "Filtered bits:      00.00\n",
      "===========================\n",
      "Round 0:\n",
      "    Input:  2^22.96 samples of 162.00 bits\n",
      "    Output: 2^29.00 samples of 107.15 bits\n",
      "                      Nullified 54.85 bits\n",
      "    t[0] = 41.92, t'[0] = 00.00\n",
      "    a[0] = 20.96, a'[0] = 00.00\n",
      "    v[0] = 20.96, v'[0] = 00.00\n",
      "    m[0] = 22.96, m'[0] = 00.00\n",
      "    n[0] = 54.85, n'[0] = 00.00\n",
      "Round 1:\n",
      "    Input:  2^29.00 samples of 107.15 bits\n",
      "    Output: 2^26.85 samples of  35.00 bits\n",
      "                      Nullified 72.15 bits\n",
      "    t[1] = 45.00, t'[1] = 00.00\n",
      "    a[1] = 27.00, a'[1] = 00.00\n",
      "    v[1] = 18.00, v'[1] = 00.00\n",
      "    m[1] = 29.00, m'[1] = 00.00\n",
      "    n[1] = 72.15, n'[1] = 00.00\n",
      "Expected peak = 0.0007279172\n",
      "Average noise = 0.0000909896\n",
      "Noise * slack = 0.0007279172\n",
      "2^45.0 * 2 round = 4041 h/CPU = 84.18 h(=3.5 days)/48 threads\n",
      "250.2 GB/node\n"
     ]
    }
   ],
   "source": [
    "# Full attack on 1-bit bias with without error; HGJ-repeat\n",
    "p = 1\n",
    "dinursolver(bitbias(1)*(p-(1-p)), 162, 2, memory=29, fftsize=35, numsig=None, time=45, slack=8, algorithm=\"HGJ-repeat\")\n",
    "estimate(45, 48, 1, 29, 29, 5000, 1.01, 1.01, 1.1, 2) # In Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved problem with HGJ-repeat!\n",
      "===========================\n",
      "Initial signatures: 2^24.21 with 0 bits of sk known\n",
      "Memory cost:        2^29.00\n",
      "Reduction time:     2^44.00\n",
      "FFT size:           2^34.00\n",
      "Filtered bits:      00.00\n",
      "===========================\n",
      "Round 0:\n",
      "    Input:  2^24.21 samples of 162.00 bits\n",
      "    Output: 2^29.00 samples of 102.59 bits\n",
      "                      Nullified 59.41 bits\n",
      "    t[0] = 44.00, t'[0] = 00.00\n",
      "    a[0] = 22.21, a'[0] = 00.00\n",
      "    v[0] = 21.79, v'[0] = 00.00\n",
      "    m[0] = 24.21, m'[0] = 00.00\n",
      "    n[0] = 59.41, n'[0] = 00.00\n",
      "Round 1:\n",
      "    Input:  2^29.00 samples of 102.59 bits\n",
      "    Output: 2^29.41 samples of  34.00 bits\n",
      "                      Nullified 68.59 bits\n",
      "    t[1] = 44.00, t'[1] = 00.00\n",
      "    a[1] = 27.00, a'[1] = 00.00\n",
      "    v[1] = 17.00, v'[1] = 00.00\n",
      "    m[1] = 29.00, m'[1] = 00.00\n",
      "    n[1] = 68.59, n'[1] = 00.00\n",
      "Expected peak = 0.0002994609\n",
      "Average noise = 0.0000374326\n",
      "Noise * slack = 0.0002994609\n",
      "2^44.0 * 2 round = 2020 h/CPU = 42.09 h(=1.8 days)/48 threads\n",
      "250.2 GB/node\n"
     ]
    }
   ],
   "source": [
    "# Attack on 1-bit bias with with 2.7% error; HGJ-repeat\n",
    "p = 0.973\n",
    "dinursolver(bitbias(1)*(p-(1-p)), 162, 2, memory=29, fftsize=34, numsig=None, time=44, slack=8, algorithm=\"HGJ-repeat\", streamout=True)\n",
    "estimate(44, 48, 1, 29, 29, 5000, 1.01, 1.01, 1.1, 2) # In 48-core workstation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved problem with HGJ-layer!\n",
      "===========================\n",
      "Initial signatures: 2^26.00 with 0 bits of sk known\n",
      "Memory cost:        2^29.00\n",
      "Reduction time:     2^44.00\n",
      "FFT size:           2^34.00\n",
      "Filtered bits:      00.00\n",
      "===========================\n",
      "Round 0:\n",
      "    Input:  2^26.00 samples of 162.00 bits\n",
      "    Output: 2^29.00 samples of 103.00 bits\n",
      "                      Nullified 59.00 bits\n",
      "    t[0] = 44.00, t'[0] = 00.00\n",
      "    a[0] = 22.00, a'[0] = 00.00\n",
      "    v[0] = 20.00, v'[0] = 00.00\n",
      "    m[0] = 26.00, m'[0] = 00.00\n",
      "    n[0] = 59.00, n'[0] = 00.00\n",
      "Round 1:\n",
      "    Input:  2^29.00 samples of 103.00 bits\n",
      "    Output: 2^29.00 samples of  34.00 bits\n",
      "                      Nullified 69.00 bits\n",
      "    t[1] = 44.00, t'[1] = 00.00\n",
      "    a[1] = 27.00, a'[1] = 00.00\n",
      "    v[1] = 17.00, v'[1] = 00.00\n",
      "    m[1] = 29.00, m'[1] = 00.00\n",
      "    n[1] = 69.00, n'[1] = 00.00\n",
      "Expected peak = 0.0002994609\n",
      "Average noise = 0.0000432123\n",
      "Noise * slack = 0.0002994609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25.998200005082655, 44.0)"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attack on 1-bit bias with with 2.7% error; HGJ-layer\n",
    "p = 0.973\n",
    "dinursolver(bitbias(1)*(p-(1-p)), 162, 2, memory=29, fftsize=34, numsig=None, time=44, slack=6.93, algorithm=\"HGJ-layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved problem with BCJ-repeat!\n",
      "===========================\n",
      "Initial signatures: 2^29.00 with 0 bits of sk known\n",
      "Memory cost:        2^26.00\n",
      "Reduction time:     2^44.00\n",
      "FFT size:           2^34.00\n",
      "Filtered bits:      03.00\n",
      "===========================\n",
      "Round 0:\n",
      "    Input:  2^26.00 samples of 159.00 bits\n",
      "    Output: 2^29.00 samples of  34.00 bits\n",
      "                      Nullified 125.00 bits\n",
      "    t[0] = 44.00, t'[0] = 00.00\n",
      "    a[0] = 22.00, a'[0] = 00.00\n",
      "    v[0] = 00.00, v'[0] = 00.00\n",
      "    m[0] = 26.00, m'[0] = 00.00\n",
      "    n[0] = 125.00, n'[0] = 00.00\n",
      "    w[0] = 00.00\n",
      "Expected peak = 0.0002994609\n",
      "Average noise = 0.0000432123\n",
      "Noise * slack = 0.0002994609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28.996400010165303, 44.0)"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attack on 1-bit bias with with 2.7% error; BCJ-repeat\n",
    "p = 0.973\n",
    "dinursolver(bitbias(1)*(p-(1-p)), 162, 1, memory=29, fftsize=34, numsig=None, time=44, slack=6.93, algorithm=\"BCJ-repeat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved problem with BCJ-layer!\n",
      "===========================\n",
      "Initial signatures: 2^29.00 with 0 bits of sk known\n",
      "Memory cost:        2^26.00\n",
      "Reduction time:     2^44.00\n",
      "FFT size:           2^34.00\n",
      "Filtered bits:      03.00\n",
      "===========================\n",
      "Round 0:\n",
      "    Input:  2^26.00 samples of 159.00 bits\n",
      "    Output: 2^29.00 samples of  34.00 bits\n",
      "                      Nullified 125.00 bits\n",
      "    t[0] = 44.00, t'[0] = 00.00\n",
      "    a[0] = 22.00, a'[0] = 00.00\n",
      "    v[0] = 00.00, v'[0] = 00.00\n",
      "    m[0] = 26.00, m'[0] = 00.00\n",
      "    n[0] = 125.00, n'[0] = 00.00\n",
      "    w[0] = 00.00\n",
      "Expected peak = 0.0002994609\n",
      "Average noise = 0.0000432123\n",
      "Noise * slack = 0.0002994609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28.996400010165303, 44.0)"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attack on 1-bit bias with with 2.7% error; BCJ-layer\n",
    "p = 0.973\n",
    "dinursolver(bitbias(1)*(p-(1-p)), 162, 1, memory=29, fftsize=34, numsig=None, time=44, slack=6.93, algorithm=\"BCJ-layer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.0",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
